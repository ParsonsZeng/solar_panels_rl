\documentclass[11pt]{article}

% --- Packages ---
\usepackage[usenames, dvipsnames]{color} % Cool colors
\usepackage{enumerate, amsmath, amsthm, amssymb, fullpage, csquotes, dashrule, tikz, bbm, booktabs, bm}
\usepackage[framemethod=TikZ]{mdframed}
\usepackage[numbers]{natbib}

% --- Misc. ---
\hbadness=10000 % No "underfull hbox" messages.
\setlength{\parindent}{0pt} % Removes all indentation.

% --- Commands ---
\input{commands}

% --- Meta Info ---
\title{Improving Solar Panels with Reinforcement Learning}
\author{Emily Reif  and David Abel \\ \texttt{\{emily\_reif@brown.edu, david\_abel@brown.edu\}} \\ Department of Computer Science, Brown University, Providence, RI 02912 }
\date{}

% --- Begin Document ---
\begin{document}
\maketitle

% -----------------
% -- Abstract --
% -----------------
\begin{abstract}
Solar panels offer a sustainable means of harvesting energy from the sun. One of the primary factors dictating the efficacy of an individual solar panel is the angle of indicidence to solar irradiance; photovoltaic cells pointed directly at the sun harvest substantially more energy than those pointed away from the sun. To maximize energy, solar panels are often equipped with a tracking system to enable efficient and accurate computation of the sun's relative location throughout the day, regardless of the panel's location on Earth. Prior work advances efficient algorithms for computing the sun's location based on the longitude and latitude of the system, the current temperature, time of day, altitude, local atmospheric pressure, among other quantities. With an accurate estimation of the sun's location, solar tracking systems point panels toward the sun to maximize their efficacy. However, these approaches do not account for subtle changes in the local climate, current weather conditions, or atmospheric composition which can be contributing factors to the total energy harvested by a solar panel. In this work, we use the computational learning paradigm of Reinforcement Learning to optimize solar panel performance, measured in terms of the total amount of AC energy harvested in a 24 hour cycle by an end to end solar system. We advocate for the use of Reinforcement Learning for the Solar Tracking problem due to its {\it effectiveness}, {\it negligible cost}, {\it lack of dependence on extra components} (such as a thermometer, barometer, or a GPS), and {\it versatility}. Our contribution is twofold: (1) the development of an RL algorithm, \textsc{SolarRL} designed specifically for optimizing the total energy harvested by solar panels, and (2) the creation of an open source simulation platform for solar tracking experimentation. We evaluate the utility of our algorithm compared to baselines in our simulated environment as well as on a fleet of real solar panels, tested during the Winter and Spring in Providence, Rhode Island.
\end{abstract}

% ----------------------
% -- Introduction --
% ----------------------
\section{Introduction}
Solar energy offer a pollution free and fully sustainable means of harvesting energy directly from the sun. Consequently, considerable effort has been directed toward maximizing the efficiency of end to end solar systems, including the design of photovoltaic cells, the layout of panels, engineering new photovoltaic architectures and materials, and solar tracking systems. Solar tracking is especially important for maximizing performance of solar panels~\cite{Eke2012,Rizk2008,King2001}. Systems equipped with a tracking mechanism given each panel one or two degrees of freedom. A tracking algorithm computes the relative location of the sun in the sky throughout the day and a controller moves the panel along its axes to point at the sun. The goal is to minimize the angle of incidence between incoming solar irradiance and the grid of photovoltatic cells, as in ~\citet{Eke2012,Benghanem2011,King2001}. Prior work has consistently demonstrated that panels using a tracking system increase the total energy by a substantial amount \dnote{Let's put an actual percentage with some citations}.

Existing solar tracking algorithms are sufficiently accurate to inform control of panels. Each of these algorithms takes as input relevant quantities to locate the panel and sun in time and space, including GPS coordinates and the year (among other things). ~\citet{reda2004solar} develop an algorithm that computes the location the two panel-relative angles needed to determine the sun's location in the sky within $\pm 0.0003$ degrees of accuracy. Further, the approach is advertised as accurate between the years 2000 B.C.E to 6000 A.D, making it robust to the movements of relevant celestial bodies for the foreseeable future, However, there are two limitations to the approach. First, it's computationally inefficient to the point of impracticality, and second, the algorithm requires a variety of data not easily available, especially in locations with large grids of solar panels. Other algorithms have addressed these limitations, focusing on creating a computationally efficient solar tracking algorithm that depends on a small number of easily accessible input data. The most widespread of these approaches is introduced by~\citet{Grena2008}, achieving computational efficiency but still depending on data like longitude, latitude, atmospheric pressure, and the temperature. Furthermore, previous studies suggest that pointing a panel directly at the sun is not always optimal~\citet{Kelly2009,Hussein1995,King2001}. Consequently, algorithms that point panels directly at the sun may be achieving sub-optimal behavior.\\

In this work, we use the computational paradigm of Reinforcement Learning (RL) to optimize solar panel performance, measured in the total amount of energy harvested in a 24 hour cycle. We advocate for the use of RL due to its {\it effectiveness}, {\it negligible cost}, {\it lack of dependence on extra data} (such as a GPS), and {\it versatility}. That is, using RL, solar control can take into account other environmental factors like temperature, cloud coverage, and atmospheric conditions. We develop a new RL algorithms, \textsc{SOLARL} designed specifically for optimizing solar energy harvesting, create a simulation platform for solar energy harvesting, and test the utility of our algorithm in this simulated environment against industrial standard algorithms and a standard state of the art supervised learning approach. We now turn to introducing relevant background.


% ----------------------
% -- Background --
% ----------------------
\section{Background}



\subsection{Solar Tracking}





\subsection{Reinforcement Learning}

Reinforcement Learning is a computational learning paradigm in which learning takes the form of purely positive and negative reinforcement.


\subsection{Why Use RL for Solar Tracking?}


% -----------------------
% -- Related Work --
% -----------------------
\section{Related Work}



In particular, the algorithm takes as input the following eight parameters:
\begin{enumerate}
\item ecliptic longitude
\item ecliptitic latitude
\item apparent right ascension
\item apprarent declination
\item nutation in longitude
\item nutation in obliquity
\item obliquity of ecliptic
\item true geometric distance
\end{enumerate}

Other algorithms have addressed thes




% -----------------------------
% -- System Overview --
% -----------------------------
\section{System Overview}



% ----------------------
% -- Experiments --
% ----------------------
\section{Experiments}


\subsection{Simulation}

\subsection{Physical System}



% ---------------------
% -- Conclusion --
% ---------------------
\section{Conclusion}


















% --- Bibliography ---
\bibliographystyle{plainnat}
\bibliography{../solar}

\end{document}